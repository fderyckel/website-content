---
title: "Disaster Tweets - Part I"
author: ''
date: '2020-05-25'
slug: disaster-tweets-part-i
categories: ["R"]
tags: ["Classification", "Lasso", "NLP", "kaggle"]
subtitle: ''
summary: ''
authors: []
lastmod: '2020-05-25T21:35:08+02:00'
featured: yes
image_preview: "featured.jpg"
header: 
  image: "featured.jpg"
  caption: 'One of our physical challenge at the KVFD'
projects: []
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
    fig_caption: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = TRUE)
```


# Introduction

*Real or Not? NLP with Disaster Tweets*  Predict which Tweets are about real disasters and which ones are not.  
The task comes from a [Kaggle competition](https://www.kaggle.com/c/nlp-getting-started) which is to detect if a tweet about an emergency disaster is real.  Hence, this is a NLP classification problem.  

It is kind of easy for a human to see if a tweet is real or not, but it is harder for machine to detect it.  For instance, the tweet *"look at the sky last night, it was ABLAZE"*.  Although there is the use of a disaster keyword like "ablaze", the use of the word in this context wasn't meant to refer to an emergency disaster.  This is seen as *"a getting started"* problem by Kaggle. 

Being a volunteer firefighter in my local community for the last 3 years, this Kaggle task struck a chord with me.  And yes, that is me on the picture.  Imagine this heavy, well insulated PPE, super intense physical challenge and then the Saudi heat with the humidity of the Red Sea ;-) 

I am planning on a 3 parts post. 

* First part is very much BOW (bag of word) approach using Lasso. 
* Second part is still BOW approaches using SVD.  Modeling with Lasso and Xgboost. 
* Third part is word embedding using Glove.  (Still trying to make it work with Bert pre-trained models.  Maybe I'll have that sort out by the end. )

Throughout these posts, I will use packages from 3 main sets: the [tidyverse](https://www.tidyverse.org/) for data wrangling, the [tidymodels](https://www.tidymodels.org/) for modeling and the [tidytext](https://www.tidytextmining.com/) for dealing with text data.  These sets of packages make a coherent whole and, in my opinion, makes it easier to learn the data analysis & modeling workflow.  It is, of course, not the only one.  There are many other alternatives in R.  

Loading the libraries first. 

```{r loading_libraries}
library(readr)      # to read and write (import / export) any type into our R console.
library(dplyr)      # for pretty much all our data wrangling
library(stringr)    # to deal with strings.  this is a NLP task, so lots of it ;-) 
library(purrr)      # to map functions over rows
library(forcats)    # to deal with categorical variables: the fct_reorder() function
library(stringr)    # to use str_remove() and many other regex functions later 
library(ggplot2)    # to plot

library(kableExtra) # for making pretty table on html

library(rsample)    # to split df with initial_split() a
                    # to use resampling techniques with bootstrap() and vfold_cv()
library(parsnip)    # the main engine that run the models 
library(workflows)  # to use workflow()
library(tune)       # to fine tune the hyperparameters using tune()
library(dials)      # to create grid of parameters using grid_regular(), tune_grid(), penalty()
library(yardstick)  # to create the measure of accuracy, f1 score and ROC-AUC 

library(glmnet)     # to use lasso, it is called autmoatically when calling set_engine() 
                    # but it isn't call later on when doing using predict()

library(vip)        # tidy framework to check variables importance

```

Without further adue, let's get started by loading our training set and check its structure.  

```{r loading-data} 
# loading our training data  
df_train <- read_csv("~/disaster_tweets/data/train.csv") %>% as_tibble()  

# let's have a look at it 
skimr::skim(df_train)
```

At first look: 7613 observations, 5 variables (one target + 4 predictors).  Many missing values for the **location** variable and a few missing on the **keyword** variable as well.  The **Text** variable (which are the tweets themselves) has 0 missing values.  Notice, we have an ID variable (don't think that it has any use).  

let's just have a look at 10 tweets and the *target* column will tell us if the tweet is being considered as one about a real emergency disaster.  

```{r first-ten-tweet, echo=FALSE}
df_train[50:60,] %>% select(target, text) %>% 
  kable("html", caption = "10 random tweets") %>% 
  kable_styling(bootstrap_options = c("striped", "hoover"), full_width = F, position = "center")
```

Because this is a classification problem, we need to make our target variable a factor.   

Alhtough, we will just use our train dataframe for modeling, we'll still split it to get a testing set from it (which we will test our models on).   

```{r splitting_data}
# The target variable should be a factor as this is classification problem. 
df_train <- df_train %>% 
  mutate(target_bin = factor(if_else(target == 1, "a_truth", "b_false"))) %>% 
  select(-target)

# Just checking how balanced is our data.  It seems well balanced. 
prop.table(table(df_train$target_bin))

# initial split with strata will keep the same proportion of target variable as in the original df. 
set.seed(0109)
rsplit_df <- initial_split(df_train, strata = target_bin, prop = 0.85)

# If we use cross-validation, we do not normally really need to do this. 
# we still check our accuracy on that set of data (our unseen data) .
df_train_tr <- training(rsplit_df)

# and just checking again about the ratio of target variable
prop.table(table(df_train_tr$target_bin))   # same as original set. 

df_train_te <- testing(rsplit_df)
```

The **initial_split()** function gives a rsplit object (rsplit_df in our case) that can be used with the training() and testing() functions to extract the data in each split.  The strata argument "help ensure that the number of data points in the training data is equivalent to the proportions in the original data set."

A good thing that I notice is the data set is well-balanced-ish with a 57% - 43% in the occurrence of the outcomes (0, 1).  So we won't need to add more/remove data in our set to over-compensate.  That's one less problem that we have to deal with. 

# Base line model - Lasso model on just text  

In this post, we skip some of the usual data exploration (wordcloud are pretty, but are there really useful?) and start straight into building a model.  This very first model, will be our base case.  We will build a Lasso classification model based just on a cleaner version of the text of the tweets.  

For a Lasso modeling task, we can only use numerical values. Then we will need to normalize them. Also, we cannot include missing value.  So, we remove these columns with missing values. So basically, we just use the text data as predictor.  We'll numerize that text column using the tf_idf.  For more on transforming text into tf_idf, you can check [this section](https://www.tidytextmining.com/tfidf.html#the-bind_tf_idf-function) of the David Robinson & Julia Silge book on *Tidy Text Mining*.  Most of the ideas here come from her book and [blog](https://juliasilge.com/).  

To clean our tweets, we will use the **recipes** and **textrecipes** packages.  So, the exact same steps can later be done more easily on the testing set. 

In order, we'll tokenize the tweets (at the same time, that will remove punctuations and lowercase all text), remove stop words, and keep only the first 1250 tokens. On the last step, we'll transform our words into numerical values by transforming them to an tf_idf.  

```{r processing_data} 
library(tidytext)
library(textrecipes)

recipe_tweet <- recipe(formula = target_bin ~ text + id, data = df_train_tr) %>% 
  update_role(id, new_role = "ID") %>% 
  step_tokenize(text) %>%        # Tokenize the tweets into words
  step_stopwords(text) %>%       # Filtering off stopwords from the tokenlist variable
  step_tokenfilter(text, max_tokens = 1250) %>%  # Only keep the 1250 most important words
  step_tfidf(text) %>%           # transform each words by its tf_idf values. 
  step_normalize(all_numeric())  # normalizing the tf_idf values
``` 

Once a recipe is written, we can check what it does to the original data frame by prepping and then juicing it. 

```{r checking_prep}  
# checking on the prep() function. 
df_train_tr_processed <- recipe_tweet %>% prep %>% juice()
dim(df_train_tr_processed)
```

Notice, we have now 1252 columns.  The id column + the target variable + the 1250 tokens. 

## Creating a model workflow   

With the new tidymodel API, you can now create a workflow for a model, that can be reused later on.  

To find the most appropriate penalty for this data set, we'll boostrap 25 samples on each penalty.  We'll do that using the **rsample** library. 

```{r creating_model} 
library(doParallel)
registerDoParallel(cores = 16)   #let's work on 16 cores. 

# defining our model.  It is a logistic regression, using glmnet. 
# notice that the penaly is set to tune()... We'll create a grid for that. 
# mixture = 1, means we are dealing with LASSO. 
model_lasso <- logistic_reg(mode = "classification", 
                            penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")

# if we are to tune the penalty parameters, let's create a grid of possible values
# we 'll assign 40 different values for our penalty. 
grid_lambda <- grid_regular(penalty(), levels = 40)

# And we'll test each penalty value on 25 bootstrap. 
## So that's like a 1000 models to fit.  
folds_training <- bootstraps(df_train_tr, strata = target_bin, times = 25)

# starting our worflow
wf_lasso <- workflow() %>% 
  add_recipe(recipe_tweet) %>% 
  add_model(model_lasso)

# the tune_grid() will fit our 1000 models using parallel processing
# we are looking at 3 measures of validity: roc, f1 and accuracy. 
tune_lasso <- tune_grid(
  wf_lasso, 
  resamples = folds_training, 
  grid = grid_lambda, 
  metrics = metric_set(roc_auc, f_meas, accuracy), 
  control   = control_grid(verbose = TRUE)
)
```

I belive that the tune_grid() function is the only place in our modeling workflow where parallel processing is being used. All the other tasks are single core processing. 

What have we done? First, we have created a model workflow, with workflow(), using a recipe for pre-processing and a model type, logistic_reg(). Second, we fine tuned the parameters of our model.  In this case, we only fine tuned the penalty parameter. 

## Analyis of results  

```{r checking-metrics_lasso} 
# we collect the results of our sample to see what penalty value we will pick. 
metrics <- tune_lasso %>% collect_metrics()

# save the metric for later use
metrics %>% write_csv("~/disaster_tweets/data/metrics_lasso_base.csv") 

metrics %>% 
  ggplot(aes(x = penalty, y = mean, color = .metric)) + 
  geom_line() + 
  facet_wrap(~.metric) + 
  scale_x_log10()
```

Using the plots, we can see that there is only a small window of penalty values that will increase the performance of of the model.  We keep that in mind for the next time we create our grid of penalties.  

Because our dataset is somehow balance, we could choose **accuracy** as a measure of model validity.  The best accuracy on this base model is 76.2 % for the training set.  That said because Kaggle choose F1 as a performance metric, let's choose the penalty with highest **f_meas** and fit it to our testing set. 

```{r metric-base-model} 
# let check the penalties values that give the best performances
metrics %>% group_by(.metric) %>% top_n(4, mean) %>% arrange(.metric, desc(mean)) %>% 
  kable("html", caption = "Penalties with best performances") %>% 
  kable_styling(bootstrap_options = c("striped", "hoover"), full_width = F, position = "center") 

```

## Picking the best model  

We'll pick the penalty that gives us the best F1 score. The finalize_workflow() functions will take the existing workflow and add to it the chosen parameters. Finally, we will save our model for later.  

```{r picking_model_lasso}  
best_metric <- tune_lasso %>% select_best("f_meas")

wf_lasso <- finalize_workflow(wf_lasso, best_metric)

# to summarize, this is how our workflow looked like 
wf_lasso

# to check our model performance on the unseen data set, we use the last_fit() 
## Notice how the last_fit() works on the rsplit object
last_fit(wf_lasso, rsplit_df) %>% collect_metrics()
# got 78% accuracy on unseen data (the df_test) with 82.9% ROC. 

# To save our model for later use, we first need to fit it, 
model_lasso_base <- fit(wf_lasso, df_train)
write_rds(x = model_lasso_base, path = "~/disaster_tweets/data/model_lasso_base.rds")
```

## variable importance  

We can also check for the important variables that determine if a tweet is about a real emergency or not.  

```{r checking-variable-importance-lasso}  
read_rds("~/disaster_tweets/data/model_lasso_base.rds") %>% 
  pull_workflow_fit() %>% 
  vi(lamda = best_metric$penalty) %>% 
  group_by(Sign) %>%
  top_n(20, wt = abs(Importance)) %>%
  ungroup() %>%
  mutate(Importance = abs(Importance), 
         Variable = str_remove(Variable, "tfidf_text_"), 
         Variable = fct_reorder(Variable, Importance)) %>%
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Sign, scales = "free_y") +
  labs(y = NULL) 

```

For some reasons, I need to change the label of each graph.  The "POS" terms will most likely give place to not a real emergency tweet.  

Although I am glad that the "lmao" expression is more often related to tweets that are not about real emergency, I am confused as to why "https" is one side and "t.co" on the other sides.  They are both about links.  🤔 

## Submission of results  

Let's now apply our base model to our test set to create the prediction (target variable). 

```{r first_submission} 
test <- read_csv("~/disaster_tweets/data/test.csv")

prediction_lasso_base <- tibble(id = test$id,  
                                prediction = predict(read_rds("~/disaster_tweets/data/model_lasso_base.rds"), new_data = test)) %>% 
  mutate(target = if_else(prediction == "a_truth", 1, 0))

prediction_lasso_base %>% select(id, target) %>% write_csv(path = "~/disaster_tweets/data/prediction_lasso_base.csv")
# this submission gave 77.8 % on the Kaggle public score. 
``` 

# Baseline with some additional features  

In this new model, we will do some feature engineering at a basic level and see if that helps to increase the model performance and especially its accuracy.  We continue to use the same lasso model.  That means everything has to be converted back to numerical variables. 

We create another version of the basic df. 
Here are the feature engineering steps we will take:

## Rebulding the data frame and variables  

In the following step, there were a lots of trials and errors in regards of the order with which we were performing the changes on the tweets. 

We add the following variables: 

* add a variable for the number of hashtag in a tweet (like #xxx) 
* add a variable for the number of http link in a tweet (like http://xxxx) 
* add a variable for the number of mention in a tweet (like @xxxx) 
* add a variable if the tweet contains a location 
* add a variable if the tweet contains a keyword 
* remove all mentions and links 
* add a variable for the number of digits in a tweet 
* add a variable for the number of character in a tweet 
* add a variable for the number of words in a tweet
* remove all the numbers in tweets

On the text itself, we perform the following steps: 

* remove all digits (although I am suspecting that 4 digits date like 2015 or 2017 could have an influence) 
* remove all mentions (who is being mentioned might add little value, we have a variable if someone is mentioned) 
* remove all http links. The link itself is not discriminatary (does not add any information). We just recorded at the previous step if we have a link, so we can delete it 
* lemmatize all words
* remove all non latin letters  
* count if there are multiple repeated characters in a row (like omggggg).  My thinking is that real emergency tweets might use less of these.  

Because of all the cleaning, better make it a function that we can apply to both training and laster on testing set. 

```{r base_with_features} 
clean_tweets <- function(file_path){
  df <- read_csv(file_path) %>% as_tibble()  %>% 
    mutate(number_hashtag = str_count(string = text, pattern = "#"), 
           number_number = str_count(string = text, pattern = "[0-9]") %>% as.numeric(), 
           number_http = str_count(string = text, pattern = "http") %>% as.numeric(), 
           number_mention = str_count(string = text, pattern = "@") %>% as.numeric(), 
           number_location = if_else(!is.na(location), 1, 0), 
           number_keyword = if_else(!is.na(keyword), 1, 0), 
           number_repeated_char = str_count(string = text, pattern = "([a-z])\\1{2}") %>% as.numeric(),  
           text = str_replace_all(string = text, pattern = "http[^[:space:]]*", replacement = ""), 
           text = str_replace_all(string = text, pattern = "@[^[:space:]]*", replacement = ""), 
           number_char = nchar(text),   #add the length of the tweet in character. 
           number_word = str_count(string = text, pattern = "\\w+"), 
           text = str_replace_all(string = text, pattern = "[0-9]", replacement = ""), 
           text = map(text, textstem::lemmatize_strings) %>% unlist(.), 
           text = map(text, function(.x) stringi::stri_trans_general(.x, "Latin-ASCII")) %>% unlist(.), 
           text = str_replace_all(string = text, pattern  = "\u0089", replacement = "")) %>% 
  select(-keyword, -location) 
  return(df)
}

df_train <- clean_tweets("~/disaster_tweets/data/train.csv") 
```

A little more checking. 

```{r skimr_lasso}
# to help me see what other changes still have to be made
yo <- df_train %>% select(id, text) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words %>% filter(lexicon == "snowball")) %>% 
  count(word) %>% arrange(desc(n))

# I wanted to check what are the different stop-words dictionary 
# and see if it could make a difference. 
yo <- stop_words %>% group_by(lexicon) %>% summarize(n = n())

# just checking if everything is as expected
skimr::skim(df_train)
```

Checking on the tweets, I can see that some digits are left.  I am not sure how that happened (any suggestion welcome!) 

Also, I notice that there are around 2000 words with n >= 6.  

Some tweets are in the dataset more than once, but ... they do not have the same target value.  Yes, that's weird and it is due to bad encoding.  So we'll use a voting sytem, to make them equal.  

```{r multiple_same_tweet} 
yo <- df_train %>% group_by(text) %>% 
  mutate(mean_target = mean(target), 
         new_target = if_else(mean_target > 0.5, 1, 0)) 

df_train <- yo %>% 
  mutate(target = new_target, 
         target_bin = factor(if_else(target == 1, "a_truth", "b_false"))) %>% 
  select(-new_target, -mean_target, -target)
```

Now, we start our modeling workflow. 

## Creating and tuning a model   

There are 2 things we'd like to try in this step.  Would lemmatize or stemming work better?  We'll try both, but keep the one with best result. Also we try to see if doing a dimensionality reductions with PCA would help. 

Also because we use tf_idf, should we normalize?  Does this step add anything on our model? 

```{r model-lasso-enhanced}  
recipe_tweet <- recipe(formula = target_bin ~ ., data = df_train) %>% 
  update_role(id, new_role = "ID") %>% 
  step_normalize(contains("number"), -id) %>% 
  step_tokenize(text) %>%        
  step_stopwords(text, stopword_source = "snowball") %>%  
  step_tokenfilter(text, max_tokens = 2500) %>%  
  step_tfidf(text) %>%    
  step_pca(contains("tfidf"), threshold = 0.95)

# to check how our df is now looking as it has been pre-processed. 
#df_train_processed <- recipe_tweet %>% prep() %>% juice()
#dim(df_train_processed)

registerDoParallel(cores = 16)

# we 'll assign 40 different values for our penalty. 
# we noticed earlier that best values are between penalties 0.001 and 0.005
grid_lambda <- expand.grid(penalty = seq(0.0017,0.005, length = 40)) 

# This time we'll use 10 folds cross-validation
set.seed(0109)
folds_training <- vfold_cv(df_train, v = 10, repeats = 2) 

model_lasso <- logistic_reg(mode = "classification", 
                            penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet") 

# starting our worflow
wf_lasso <- workflow() %>% 
  add_recipe(recipe_tweet) %>% 
  add_model(model_lasso) 

# run a lasso regression with bootstrap, on 40 different levels of penalty
tune_lasso <- tune_grid(
  wf_lasso, 
  resamples = folds_training, 
  grid = grid_lambda, 
  metrics = metric_set(roc_auc, f_meas, accuracy), 
  control = control_grid(verbose = TRUE)
) 

tune_lasso %>% collect_metrics() %>% 
  write_csv("~/disaster_tweets/data/metrics_lasso_enhanced.csv") 

tune_lasso %>% collect_metrics() %>% 
  group_by(.metric) %>% top_n(4, mean) %>% arrange(.metric, desc(mean)) %>% 
  kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hoover"), full_width = F, position = "center") 
```

Note1: the transformed df after the recipes steps is 7613 x 1237, that is with 1750 maxtokens and pca on 95% variance.  Also we only do PCA on the words (not the extra variable)  accuracy = 79.8 and f1 = 74.6. 

Note2: And with 2000 maxtokens, 95% threshold on pca, no nomralization we got 79.8% accuracy and f1 = 74.9.  df is 7613 x 1369. 

Note3: And with 1750 maxtokens, no pca, we got 79.9% accuracy ad f1 = 74.2. 

Note4: And with 2500 maxtokens, 95% threshold on pca, no nomralization we got 80.0% accuracy and f1 = 74.98.  df is 7613 x 1630.  And we get the exact same results if we use normalization. 

All this fine tuning gave us a very small increase in both accuracy and F1 scores.   There is also an increase in ROC-AUC.  Considering these are the performances on the trained data, and considering the leakage due to normalization, pca and tf-idf, these might be optimist results.   Let's see.  

```{r fitting_lasso_withfeatures} 
best_metric <- tune_lasso %>% select_best("f_meas")

wf_lasso <- finalize_workflow(wf_lasso, best_metric)

wf_lasso

#save the final lasso model
fit(wf_lasso, df_train) %>% write_rds(path = "~/disaster_tweets/data/model_lasso_enhanced.rds") 
```

## Variable importances 

```{r vip-lasso-enhanced, fig.cap='Most important variables.'}  
library(vip)

read_rds("~/disaster_tweets/data/model_lasso_enhanced.rds") %>% pull_workflow_fit() %>% 
  vi(lambda = best_metric$penalty) %>% 
  group_by(Sign) %>%
  top_n(20, wt = abs(Importance)) %>%
  ungroup() %>%
  mutate(Importance = abs(Importance), 
         #Variable = str_remove(Variable, "tfidf_text_"), 
         Variable = fct_reorder(Variable, Importance)) %>%
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Sign, scales = "free_y") +
  labs(y = NULL, x = "Sign")  
```

One thing worth noticing is that not many of our feature engineering made it to the top 20 of important variables.  
What about all our fancy extra variables like number of character? number of http link? Number of # and @?  
Well ... 

```{r feature_enginee}
read_rds("~/disaster_tweets/data/model_lasso_enhanced.rds") %>% pull_workflow_fit() %>% 
  vi(lambda = best_metric$penalty) %>% 
  filter(str_detect(Variable, pattern = "number")) %>% 
  arrange(desc(Importance))
```

Ugh! They are not looking that important!  That's pretty sad,  I though I was becoming THE feature engineer guy of NLP.  Nope! just humble pie instead... 

## Submission of results  

Applying the model on the test data.  We first need to reprocessed the data.  

```{r sub_lasso_enh} 
test <- clean_tweets("~/disaster_tweets/data/test.csv") 

model_lasso_enhanced <- read_rds("~/disaster_tweets/data/model_lasso_enhanced.rds")

library(glmnet)

prediction_lasso_enhanced <- tibble(id = test$id, 
                                    prediction = predict(model_lasso_enhanced, new_data = test)) %>% 
  mutate(target = if_else(prediction == "a_truth", 1, 0))

write_csv(prediction_lasso_enhanced %>% select(id, target), path = "~/disaster_tweets/data/prediction_lasso_enhanced.csv")

rm(list = ls())
```

Note 1: maxtokens - 1750, 95% threshold for PCA, I got 76.8% public score.  

Note 2: with maxtoken = 2000 and 95% treshold for PCA, I got actually a worst accuracy score.   

Note 3: majority voting, maxtokens = 1750 and NO PCA, result with 78.7%.  

Note 4: majority voting, maxtokens = 2500, pca at 95% (df is 1630 wide), results with 78.3% (exact same results with normalization). 

Note 5: majority voting, maxtokens = 4000, pca at 90%.  Got 78.3% public score. 

Yep!  That second attempt at features engeneering is just a little success. It only added 1% accuracy in the submission. It took a lot of work to make it happen but brought not much increase in F1 score.  It's part of the game!  

We need to use another method to numerize our tweets.  This is what we'll do in Part II and III as we consider SVD and word-embedding. 

I am looking forward for comments / feedback. 

# Wonderings and lessons learned. 

There are a few things I still wonder how to improve in the modelling workflow: 

* the relationship between tf_idf and max-tokens in recipe.  It seemed that increasing the max_tokens before the tf_idf step didn't add much values. I am wondering until what point that is the case 
* can we parallelize the map functions from the purrr package?  It takes quite a bit of times for instance to do lemmatize each tweets using map(text, textstemm::lemmatize()).  I do think that might be possible. 
* can we parallelize the recipe() %>% prep() %>% juice()?  I also find that steps very slow. I do not think that it is possible. The only reason I like to run that line is to check that the recipe steps are doing what I intended them to do 

Any feedback/advises on the 2 things above are really welcome ;-) 

Lessons learned: 

* the easiest model in the second lasso was the best.  All the other fanciers, more computationally intensed attempts didn't provide better results. 
* normalization has been recommended on Lasso models.  In our case, normalizing after the tf_idf step or after the pca didn't change much to our model.  So in regards to being parsimonious, I would say I can skip these steps in the future. 

# References 

I have copied ideas from several Kaggle notebooks and blogs.  

* Extensive use of the **textrecipes** library.  Go check his posts on [his blog](https://www.hvitfeldt.me/post/)
* To get ideas on how to use Lasso modeling on a NLP task using the tidymodel framework.  Julia Silge blog: [Sentiment analysis with tidymodels and #TidyTuesday Animal Crossing reviews](https://juliasilge.com/blog/animal-crossing/)
* To get ideas to feature engineer the original tweets[NLP with Disaster - EDA | DFM | SVD | Ensemble](https://www.kaggle.com/barun2104/nlp-with-disaster-eda-dfm-svd-ensemble) 





